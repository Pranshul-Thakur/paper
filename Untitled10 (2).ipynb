{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import io\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ],
      "metadata": {
        "id": "I37tgqRbQBoW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "ptEVRz-tjbpL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/gene_attribute_matrix.txt', 'r', encoding='latin-1') as f:\n",
        "  data = f.readlines()"
      ],
      "metadata": {
        "id": "94QGMzLoOIUd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_start_idx = None\n",
        "for i, line in enumerate(data):\n",
        "    if line.startswith('!series_matrix_table_begin'):\n",
        "        data_start_idx = i + 1\n",
        "        break\n",
        "\n",
        "data_end_idx = None\n",
        "for i, line in enumerate(data):\n",
        "    if line.startswith('!series_matrix_table_end'):\n",
        "        data_end_idx = i\n",
        "        break\n",
        "\n",
        "matrix_lines = data[data_start_idx:data_end_idx]\n",
        "matrix_data = ''.join(matrix_lines)\n",
        "df = pd.read_csv(io.StringIO(matrix_data), sep='\\t', index_col=0)"
      ],
      "metadata": {
        "id": "XZaE2qrHOV9b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mastocytosis_terms = ['mastocytosis', 'mast cell', 'systemic mastocytosis']\n",
        "\n",
        "for term in mastocytosis_terms:\n",
        "    matches = [col for col in df.columns if term.lower() in col.lower()]\n",
        "    if matches:\n",
        "        print(f\"'{term}': {matches}\")\n",
        "    else:\n",
        "        print(f\"'{term}': No matches found\")"
      ],
      "metadata": {
        "id": "ROfdLQFtHnzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e611bd-962e-4b13-9b87-1823f4900833"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'mastocytosis': ['diffuse cutaneous mastocytosis', 'aggressive systemic mastocytosis', 'mastocytosis', 'cutaneous mastocytosis', 'systemic mastocytosis', 'indolent systemic mastocytosis']\n",
            "'mast cell': ['mast cell neoplasm']\n",
            "'systemic mastocytosis': ['aggressive systemic mastocytosis', 'systemic mastocytosis', 'indolent systemic mastocytosis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nUnique values in 'Disease' column:\")\n",
        "print(df['Disease'].value_counts())"
      ],
      "metadata": {
        "id": "FbJTNhx600_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71566b79-e8f7-4187-ec8e-3f2d847fc4bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unique values in 'Disease' column:\n",
            "Disease\n",
            "7008         1\n",
            "DOID         1\n",
            "GeneID/NA    1\n",
            "80059        1\n",
            "22847        1\n",
            "            ..\n",
            "9721         1\n",
            "29035        1\n",
            "145858       1\n",
            "460          1\n",
            "129684       1\n",
            "Name: count, Length: 15311, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mastocytosis_columns = [\n",
        "    'systemic mastocytosis',\n",
        "    'indolent systemic mastocytosis',\n",
        "    'aggressive systemic mastocytosis',\n",
        "    'cutaneous mastocytosis',\n",
        "    'diffuse cutaneous mastocytosis',\n",
        "    'mastocytosis',\n",
        "    'mast cell neoplasm'\n",
        "]"
      ],
      "metadata": {
        "id": "kd2YDfaNMQ7i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mastocytosis_data = df[mastocytosis_columns]\n",
        "print(\"Mastocytosis data shape:\", mastocytosis_data.shape)\n",
        "print(\"\\nMastocytosis data summary:\")\n",
        "print(mastocytosis_data.describe())"
      ],
      "metadata": {
        "id": "PDzzcWXy063H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962c3e05-22cb-49d9-e2e9-11d72e39b0a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mastocytosis data shape: (15311, 7)\n",
            "\n",
            "Mastocytosis data summary:\n",
            "        systemic mastocytosis  indolent systemic mastocytosis  \\\n",
            "count                 15311.0                         15311.0   \n",
            "unique                    5.0                             5.0   \n",
            "top                       0.0                             0.0   \n",
            "freq                  15015.0                         15126.0   \n",
            "\n",
            "        aggressive systemic mastocytosis  cutaneous mastocytosis  \\\n",
            "count                            15311.0                 15311.0   \n",
            "unique                               5.0                     5.0   \n",
            "top                                  0.0                     0.0   \n",
            "freq                             15128.0                 15003.0   \n",
            "\n",
            "        diffuse cutaneous mastocytosis  mastocytosis  mast cell neoplasm  \n",
            "count                          15311.0       15311.0             15311.0  \n",
            "unique                             5.0           5.0                 5.0  \n",
            "top                                0.0           0.0                 0.0  \n",
            "freq                           15177.0       14826.0             15013.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nNon-zero values per mastocytosis type:\")\n",
        "for col in mastocytosis_columns:\n",
        "    non_zero_count = (df[col] != 0).sum()\n",
        "    print(f\"{col}: {non_zero_count} non-zero values\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL2vGsAJ0u0y",
        "outputId": "4cfa6773-f0ba-437b-f97b-d65c51f6a66c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Non-zero values per mastocytosis type:\n",
            "systemic mastocytosis: 296 non-zero values\n",
            "indolent systemic mastocytosis: 185 non-zero values\n",
            "aggressive systemic mastocytosis: 183 non-zero values\n",
            "cutaneous mastocytosis: 308 non-zero values\n",
            "diffuse cutaneous mastocytosis: 134 non-zero values\n",
            "mastocytosis: 485 non-zero values\n",
            "mast cell neoplasm: 298 non-zero values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "    metadata_cols = ['#.1', 'Disease']\n",
        "    data_cols = [col for col in df.columns if col not in metadata_cols]\n",
        "    df_clean = df.copy()\n",
        "    for col in data_cols:\n",
        "        df_clean[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "    print(f\"Data cleaning complete. Shape: {df_clean.shape}\")\n",
        "    return df_clean\n",
        "\n",
        "df_clean = clean_data(df)"
      ],
      "metadata": {
        "id": "N2V2XMib1WBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mastocytosis_genes(df, threshold=0.0):\n",
        "    mastocytosis_genes = df[df[mastocytosis_columns].abs().max(axis=1) > threshold]\n",
        "\n",
        "    print(f\"Found {len(mastocytosis_genes)} genes associated with mastocytosis\")\n",
        "    return mastocytosis_genes, mastocytosis_columns"
      ],
      "metadata": {
        "id": "ccGPPpJq1o3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_classification_data(df):\n",
        "    mastocytosis_mask = df[mastocytosis_columns].abs().max(axis=1) > 0\n",
        "    mastocytosis_genes = df[mastocytosis_mask].copy()\n",
        "\n",
        "    print(f\"Found {len(mastocytosis_genes)} genes associated with mastocytosis\")\n",
        "\n",
        "    y_binary = (mastocytosis_genes[mastocytosis_columns].abs().max(axis=1) > 0).astype(int)\n",
        "\n",
        "\n",
        "    y_multiclass = mastocytosis_genes[mastocytosis_columns].abs().idxmax(axis=1)\n",
        "\n",
        "    feature_columns = [col for col in df.columns if col not in mastocytosis_columns + ['#.1', 'Disease']]\n",
        "    X = mastocytosis_genes[feature_columns]\n",
        "\n",
        "    print(f\"Features shape: {X.shape}\")\n",
        "    print(f\"Binary target distribution:\\n{y_binary.value_counts()}\")\n",
        "    print(f\"Multi-class target distribution:\\n{y_multiclass.value_counts()}\")\n",
        "\n",
        "    return X, y_binary, y_multiclass, mastocytosis_genes"
      ],
      "metadata": {
        "id": "r2gQ0WJC2A-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in mastocytosis_columns:\n",
        "    print(f\"{col}: {df[col].dtype}\")\n",
        "    print(f\"  Sample values: {df[col].unique()[:10]}\")\n",
        "    print(f\"  Value counts: {df[col].value_counts().head()}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "ILLbCa6P2Oy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_nonzero, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "qT5OVEn1NA-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_multiclass = mastocytosis_genes[mastocytosis_columns].abs().idxmax(axis=1)\n",
        "\n",
        "print(f\"Multi-class target distribution:\\n{y_multiclass.value_counts()}\")\n",
        "\n",
        "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(\n",
        "    X_nonzero, y_multiclass, test_size=0.2, random_state=42, stratify=y_multiclass\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_mc_scaled = scaler.fit_transform(X_train_mc)\n",
        "X_test_mc_scaled = scaler.transform(X_test_mc)\n",
        "\n",
        "print(f\"\\nMulti-class training set: {X_train_mc.shape}\")\n",
        "print(f\"Multi-class test set: {X_test_mc.shape}\")\n",
        "print(f\"Training target distribution:\\n{y_train_mc.value_counts()}\")\n",
        "\n",
        "print(\"\\nTraining multi-class classification models...\")\n",
        "\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
        "    'SVM': SVC(kernel='rbf', random_state=42, class_weight='balanced', probability=True),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=3)\n",
        "}\n",
        "\n",
        "model_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {name}...\")\n",
        "\n",
        "    try:\n",
        "        model.fit(X_train_mc_scaled, y_train_mc)\n",
        "        y_pred = model.predict(X_test_mc_scaled)\n",
        "        accuracy = accuracy_score(y_test_mc, y_pred)\n",
        "        cv_scores = cross_val_score(model, X_train_mc_scaled, y_train_mc, cv=3, scoring='accuracy')\n",
        "\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Cross-validation Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "        model_results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'cv_score': cv_scores.mean(),\n",
        "            'cv_std': cv_scores.std(),\n",
        "            'predictions': y_pred\n",
        "        }\n",
        "\n",
        "        print(f\"\\nClassification Report for {name}:\")\n",
        "        print(classification_report(y_test_mc, y_pred))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error training {name}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if model_results:\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Model': list(model_results.keys()),\n",
        "        'Test_Accuracy': [results['accuracy'] for results in model_results.values()],\n",
        "        'CV_Mean': [results['cv_score'] for results in model_results.values()],\n",
        "        'CV_Std': [results['cv_std'] for results in model_results.values()]\n",
        "    })\n",
        "    comparison_df = comparison_df.sort_values('Test_Accuracy', ascending=False)\n",
        "    print(comparison_df)\n",
        "else:\n",
        "    print(\"No models were successfully trained.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if 'Random Forest' in model_results:\n",
        "    rf_model = model_results['Random Forest']['model']\n",
        "    if hasattr(rf_model, 'feature_importances_'):\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': X_train_mc.columns,\n",
        "            'importance': rf_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        print(\"Top 20 Most Important Features (Random Forest):\")\n",
        "        print(feature_importance.head(20))\n",
        "    else:\n",
        "        print(\"Feature importance not available for this model type.\")\n",
        "else:\n",
        "    print(\"Random Forest model not available for feature importance analysis.\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"DETAILED ANALYSIS - BEST MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if model_results:\n",
        "    best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])\n",
        "    best_model = model_results[best_model_name]['model']\n",
        "    best_predictions = model_results[best_model_name]['predictions']\n",
        "\n",
        "    print(f\"Best Model: {best_model_name}\")\n",
        "    print(f\"Test Accuracy: {model_results[best_model_name]['accuracy']:.4f}\")\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test_mc, best_predictions)\n",
        "    print(cm)\n",
        "\n",
        "    print(f\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(y_test_mc, best_predictions))\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MISCLASSIFICATION ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if model_results:\n",
        "    best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])\n",
        "    best_predictions = model_results[best_model_name]['predictions']\n",
        "\n",
        "    misclassified_mask = y_test_mc != best_predictions\n",
        "    misclassified_indices = y_test_mc[misclassified_mask].index\n",
        "\n",
        "    if len(misclassified_indices) > 0:\n",
        "        print(f\"Number of misclassified genes: {len(misclassified_indices)}\")\n",
        "\n",
        "        print(\"\\nMisclassification patterns:\")\n",
        "        misclass_df = pd.DataFrame({\n",
        "            'Actual': y_test_mc[misclassified_mask],\n",
        "            'Predicted': best_predictions[misclassified_mask]\n",
        "        })\n",
        "        print(misclass_df.value_counts())\n",
        "    else:\n",
        "        print(\"Perfect classification! No misclassified genes.\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(\"SAVING CLASSIFICATION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if model_results:\n",
        "    best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])\n",
        "    best_model = model_results[best_model_name]['model']\n",
        "\n",
        "    model_filename = f'best_mastocytosis_classifier_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
        "    joblib.dump(best_model, model_filename)\n",
        "    print(f\"Best model saved as: {model_filename}\")\n",
        "\n",
        "    scaler_filename = 'mastocytosis_classification_scaler.pkl'\n",
        "    joblib.dump(scaler, scaler_filename)\n",
        "    print(f\"Feature scaler saved as: {scaler_filename}\")\n",
        "\n",
        "    feature_names_filename = 'mastocytosis_classification_features.pkl'\n",
        "    joblib.dump(list(X_train_mc.columns), feature_names_filename)\n",
        "    print(f\"Feature names saved as: {feature_names_filename}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CLASSIFICATION ANALYSIS COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Best performing model: {best_model_name}\")\n",
        "    print(f\"Test accuracy: {model_results[best_model_name]['accuracy']:.4f}\")\n",
        "    print(f\"Cross-validation score: {model_results[best_model_name]['cv_score']:.4f}\")"
      ],
      "metadata": {
        "id": "LwXfdFufQDLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrices_enhanced(model_results, y_test_mc, title=\"Multi-class Classification\"):\n",
        "    n_models = len(model_results)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for idx, (name, results) in enumerate(model_results.items()):\n",
        "        if idx >= 4:\n",
        "            break\n",
        "\n",
        "        cm = confusion_matrix(y_test_mc, results['predictions'])\n",
        "        im = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                        ax=axes[idx], cbar=True,\n",
        "                        xticklabels=True, yticklabels=True)\n",
        "\n",
        "        axes[idx].set_title(f'{name}\\nAccuracy: {results[\"accuracy\"]:.4f}', fontsize=12, fontweight='bold')\n",
        "        axes[idx].set_xlabel('Predicted', fontsize=10)\n",
        "        axes[idx].set_ylabel('Actual', fontsize=10)\n",
        "        axes[idx].tick_params(axis='x', rotation=45)\n",
        "        axes[idx].tick_params(axis='y', rotation=0)\n",
        "\n",
        "    for idx in range(len(model_results), 4):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.suptitle(f'Confusion Matrices - {title}', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "U-z4qkNfHheR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_comparison(model_results):\n",
        "    models = list(model_results.keys())\n",
        "    accuracies = [results['accuracy'] for results in model_results.values()]\n",
        "    cv_means = [results['cv_score'] for results in model_results.values()]\n",
        "    cv_stds = [results['cv_std'] for results in model_results.values()]\n",
        "\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    bars1 = ax1.bar(models, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(models)])\n",
        "    ax1.set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    for bar, acc in zip(bars1, accuracies):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    bars2 = ax2.bar(models, cv_means, yerr=cv_stds,\n",
        "                   color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(models)],\n",
        "                   capsize=5)\n",
        "    ax2.set_title('Cross-Validation Scores (Â±1 std)', fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylabel('CV Score')\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    for bar, cv_mean in zip(bars2, cv_means):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{cv_mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    ax3.scatter(cv_means, accuracies, s=100, alpha=0.7,\n",
        "               c=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(models)])\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        ax3.annotate(model, (cv_means[i], accuracies[i]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "    ax3.set_xlabel('Cross-Validation Score')\n",
        "    ax3.set_ylabel('Test Accuracy')\n",
        "    ax3.set_title('Test Accuracy vs CV Score', fontsize=14, fontweight='bold')\n",
        "    ax3.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    ax4.axis('tight')\n",
        "    ax4.axis('off')\n",
        "\n",
        "    table_data = []\n",
        "    for model, results in model_results.items():\n",
        "        table_data.append([\n",
        "            model,\n",
        "            f\"{results['accuracy']:.4f}\",\n",
        "            f\"{results['cv_score']:.4f}\",\n",
        "            f\"Â±{results['cv_std']:.3f}\"\n",
        "        ])\n",
        "\n",
        "    table = ax4.table(cellText=table_data,\n",
        "                     colLabels=['Model', 'Test Acc', 'CV Mean', 'CV Std'],\n",
        "                     cellLoc='center',\n",
        "                     loc='center')\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(12)\n",
        "    table.scale(1.2, 1.5)\n",
        "    ax4.set_title('Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ekRQxFSU0QfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_importance_enhanced(model_results, X_train_mc, top_n=20):\n",
        "    if 'Random Forest' not in model_results:\n",
        "        print(\"Random Forest not available for feature importance analysis\")\n",
        "        return\n",
        "\n",
        "    rf_model = model_results['Random Forest']['model']\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train_mc.columns,\n",
        "        'importance': rf_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "    top_features = feature_importance.head(top_n)\n",
        "    bars = ax1.barh(range(len(top_features)), top_features['importance'],\n",
        "                   color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))\n",
        "\n",
        "    ax1.set_yticks(range(len(top_features)))\n",
        "    ax1.set_yticklabels(top_features['feature'], fontsize=10)\n",
        "    ax1.set_xlabel('Feature Importance')\n",
        "    ax1.set_title(f'Top {top_n} Most Important Features\\n(Random Forest)', fontsize=14, fontweight='bold')\n",
        "    ax1.invert_yaxis()\n",
        "\n",
        "    for i, (bar, importance) in enumerate(zip(bars, top_features['importance'])):\n",
        "        ax1.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
        "                f'{importance:.4f}', va='center', fontsize=9)\n",
        "\n",
        "    ax2.hist(feature_importance['importance'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    ax2.axvline(feature_importance['importance'].mean(), color='red', linestyle='--',\n",
        "               label=f'Mean: {feature_importance[\"importance\"].mean():.4f}')\n",
        "    ax2.set_xlabel('Feature Importance')\n",
        "    ax2.set_ylabel('Number of Features')\n",
        "    ax2.set_title('Feature Importance Distribution', fontsize=14, fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"TOP {top_n} MOST IMPORTANT FEATURES (RANDOM FOREST)\")\n",
        "    print(\"=\"*60)\n",
        "    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
        "        print(f\"{i:2d}. {row['feature']:<40} {row['importance']:.6f}\")"
      ],
      "metadata": {
        "id": "sgDameOG0RVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class_distribution_analysis(y_test_mc, y_multiclass):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    train_counts = y_multiclass.value_counts()\n",
        "    ax1.pie(train_counts.values, labels=train_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "    ax1.set_title(f'Training Set Class Distribution\\n(Total: {len(y_multiclass)} samples)',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "    test_counts = y_test_mc.value_counts()\n",
        "    ax2.pie(test_counts.values, labels=test_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "    ax2.set_title(f'Test Set Class Distribution\\n(Total: {len(y_test_mc)} samples)',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Training set:\")\n",
        "    for class_name, count in train_counts.items():\n",
        "        print(f\"  {class_name}: {count} ({count/len(y_multiclass)*100:.1f}%)\")\n",
        "\n",
        "    print(\"\\nTest set:\")\n",
        "    for class_name, count in test_counts.items():\n",
        "        print(f\"  {class_name}: {count} ({count/len(y_test_mc)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "zYdMRCZ_0VVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_detailed_misclassification_analysis(model_results, y_test_mc):\n",
        "    best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])\n",
        "    best_predictions = model_results[best_model_name]['predictions']\n",
        "\n",
        "    misclass_df = pd.DataFrame({\n",
        "        'Actual': y_test_mc,\n",
        "        'Predicted': best_predictions\n",
        "    })\n",
        "\n",
        "    cm = confusion_matrix(y_test_mc, best_predictions)\n",
        "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
        "                xticklabels=True, yticklabels=True)\n",
        "    ax1.set_title(f'Confusion Matrix - Absolute Counts\\n({best_model_name})',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Predicted')\n",
        "    ax1.set_ylabel('Actual')\n",
        "\n",
        "    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Reds', ax=ax2,\n",
        "                xticklabels=True, yticklabels=True)\n",
        "    ax2.set_title(f'Confusion Matrix - Percentages\\n({best_model_name})',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Predicted')\n",
        "    ax2.set_ylabel('Actual')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    misclassified_mask = y_test_mc != best_predictions\n",
        "    if misclassified_mask.sum() > 0:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"MISCLASSIFICATION PATTERNS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        misclass_patterns = misclass_df[misclassified_mask].groupby(['Actual', 'Predicted']).size()\n",
        "        for (actual, predicted), count in misclass_patterns.items():\n",
        "            print(f\"{actual} â†’ {predicted}: {count} cases\")\n",
        "    else:\n",
        "        print(\"\\nPerfect classification! No misclassifications found.\")"
      ],
      "metadata": {
        "id": "GZ7JiUJb0Y1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nGenerating model comparison plots...\")\n",
        "plot_model_comparison(model_results)\n",
        "\n",
        "print(\"\\nGenerating enhanced confusion matrices...\")\n",
        "plot_confusion_matrices_enhanced(model_results, y_test_mc, \"Mastocytosis Subtype Classification\")\n",
        "\n",
        "print(\"\\nGenerating feature importance analysis...\")\n",
        "plot_feature_importance_enhanced(model_results, X_train_mc, top_n=25)\n",
        "\n",
        "print(\"\\nGenerating class distribution analysis...\")\n",
        "plot_class_distribution_analysis(y_test_mc, y_multiclass)\n",
        "\n",
        "print(\"\\nGenerating misclassification analysis...\")\n",
        "plot_detailed_misclassification_analysis(model_results, y_test_mc)\n",
        "\n",
        "print(\"\\nALL VISUALIZATIONS COMPLETE!\")\n",
        "print(\"Generated plots:\")\n",
        "print(\"â€¢ Model performance comparison\")\n",
        "print(\"â€¢ Enhanced confusion matrices\")\n",
        "print(\"â€¢ Feature importance analysis\")\n",
        "print(\"â€¢ Class distribution analysis\")\n",
        "print(\"â€¢ Detailed misclassification analysis\")"
      ],
      "metadata": {
        "id": "V6Pfv6mC0dZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=256, attention_dim=128):\n",
        "        super(AttentionClassifier, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.feature_encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        self.attention_weights = nn.Linear(hidden_dim, attention_dim)\n",
        "        self.attention_context = nn.Linear(attention_dim, 1, bias=False)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded_features = self.feature_encoder(x)\n",
        "\n",
        "        attention_scores = self.attention_weights(encoded_features)\n",
        "        attention_scores = torch.tanh(attention_scores)\n",
        "        attention_weights = self.attention_context(attention_scores)\n",
        "        attention_weights = F.softmax(attention_weights, dim=1)\n",
        "\n",
        "        attended_features = encoded_features * attention_weights\n",
        "\n",
        "        output = self.classifier(attended_features)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "uIx1eObFB-Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_attention_model(X_train, y_train, X_test, y_test, num_epochs=100, batch_size=32, learning_rate=0.001):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "    y_train_tensor = torch.LongTensor(y_train_encoded).to(device)\n",
        "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "    y_test_tensor = torch.LongTensor(y_test_encoded).to(device)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "\n",
        "    model = AttentionClassifier(input_dim, num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    print(f\"\\nTraining Attention-Based Deep Learning Classifier...\")\n",
        "    print(f\"Model architecture: {input_dim} -> {model.hidden_dim} -> {num_classes} classes\")\n",
        "    print(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, attention_weights = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_predictions += (predicted == batch_y).sum().item()\n",
        "            total_samples += batch_y.size(0)\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        accuracy = correct_predictions / total_samples\n",
        "\n",
        "        train_losses.append(avg_loss)\n",
        "        train_accuracies.append(accuracy)\n",
        "\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs, test_attention = model(X_test_tensor)\n",
        "        _, test_predicted = torch.max(test_outputs.data, 1)\n",
        "        test_accuracy = (test_predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
        "\n",
        "        test_pred_labels = label_encoder.inverse_transform(test_predicted.cpu().numpy())\n",
        "\n",
        "    return model, test_accuracy, test_pred_labels, test_attention, label_encoder, train_losses, train_accuracies"
      ],
      "metadata": {
        "id": "J4WIdDPoi9fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_attention_importance(model, X_test, feature_names, top_k=20):\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "        _, attention_weights = model(X_test_tensor)\n",
        "\n",
        "        avg_attention = attention_weights.mean(dim=0).cpu().numpy().flatten()\n",
        "\n",
        "        # Debugging print statements\n",
        "        print(f\"Length of feature_names: {len(feature_names)}\")\n",
        "        print(f\"Length of avg_attention: {len(avg_attention)}\")\n",
        "\n",
        "\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'attention_weight': avg_attention\n",
        "        }).sort_values('attention_weight', ascending=False)\n",
        "\n",
        "        return feature_importance.head(top_k)\n",
        "\n",
        "print(\"\\nTraining Deep Learning Models with Attention Mechanism...\")\n",
        "\n",
        "attention_model, attention_accuracy, attention_predictions, attention_weights, label_encoder, train_losses, train_accs = train_attention_model(\n",
        "    X_train_mc_scaled, y_train_mc, X_test_mc_scaled, y_test_mc,\n",
        "    num_epochs=150, batch_size=16, learning_rate=0.001\n",
        ")"
      ],
      "metadata": {
        "id": "naEyOl-_jG03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traditional_models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
        "    'SVM': SVC(kernel='rbf', random_state=42, class_weight='balanced', probability=True),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=3)\n",
        "}\n",
        "\n",
        "model_results = {}\n",
        "\n",
        "for name, model in traditional_models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    try:\n",
        "        model.fit(X_train_mc_scaled, y_train_mc)\n",
        "        y_pred = model.predict(X_test_mc_scaled)\n",
        "        accuracy = accuracy_score(y_test_mc, y_pred)\n",
        "        cv_scores = cross_val_score(model, X_train_mc_scaled, y_train_mc, cv=3, scoring='accuracy')\n",
        "\n",
        "        model_results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'cv_score': cv_scores.mean(),\n",
        "            'cv_std': cv_scores.std(),\n",
        "            'predictions': y_pred\n",
        "        }\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Cross-validation Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error training {name}: {e}\")\n",
        "        continue\n",
        "\n",
        "model_results['Attention Deep Learning'] = {\n",
        "    'model': attention_model,\n",
        "    'accuracy': attention_accuracy,\n",
        "    'cv_score': attention_accuracy,\n",
        "    'cv_std': 0.0,\n",
        "    'predictions': attention_predictions\n",
        "}\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ENHANCED MODEL COMPARISON WITH DEEP LEARNING\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "comparison_data = []\n",
        "for name, results in model_results.items():\n",
        "    comparison_data.append({\n",
        "        'Model': name,\n",
        "        'Test_Accuracy': results['accuracy'],\n",
        "        'CV_Mean': results['cv_score'],\n",
        "        'CV_Std': results['cv_std']\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data).sort_values('Test_Accuracy', ascending=False)\n",
        "print(comparison_df)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ATTENTION-BASED FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "attention_importance = get_feature_attention_importance(\n",
        "    attention_model, X_test_mc_scaled, X_train_mc.columns, top_k=25\n",
        ")\n",
        "\n",
        "print(\"Top 25 Most Important Features (Attention Mechanism):\")\n",
        "for i, (_, row) in enumerate(attention_importance.iterrows(), 1):\n",
        "    print(f\"{i:2d}. {row['feature']:<40} {row['attention_weight']:.6f}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"DEEP LEARNING MODEL ANALYSIS\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Attention-Based Deep Learning Accuracy: {attention_accuracy:.4f}\")\n",
        "print(f\"Model Parameters: {sum(p.numel() for p in attention_model.parameters()):,}\")\n",
        "print(f\"Trainable Parameters: {sum(p.numel() for p in attention_model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "print(\"\\nClassification Report (Attention Model):\")\n",
        "print(classification_report(y_test_mc, attention_predictions))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(train_accs)\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "models_comp = [name for name in model_results.keys()]\n",
        "accuracies_comp = [results['accuracy'] for results in model_results.values()]\n",
        "colors = ['red' if 'Attention' in name else 'skyblue' for name in models_comp]\n",
        "\n",
        "plt.bar(models_comp, accuracies_comp, color=colors)\n",
        "plt.title('Model Comparison: Traditional vs Deep Learning')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.xticks(rotation=45)\n",
        "for i, acc in enumerate(accuracies_comp):\n",
        "    plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"ðŸ§  DEEP LEARNING ANALYSIS COMPLETE!\")\n",
        "print(f\"{'='*50}\")\n",
        "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])\n",
        "print(f\"ðŸ† Best Model: {best_model_name}\")\n",
        "print(f\"ðŸ“Š Best Accuracy: {model_results[best_model_name]['accuracy']:.4f}\")\n",
        "\n",
        "if 'Attention' in best_model_name:\n",
        "    print(\"ðŸŽ¯ Novel deep learning approach outperformed traditional methods!\")\n",
        "    print(\"âœ¨ Attention mechanism provides interpretable feature importance\")\n",
        "else:\n",
        "    print(\"ðŸ“ˆ Traditional ML competitive, but attention provides interpretability\")"
      ],
      "metadata": {
        "id": "3wuzH_AZjWoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U3l2Xhs4j6DF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}